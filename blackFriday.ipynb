{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550068"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "train=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\projects\\train_oSwQCTC\\train.csv')\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|           User_ID|Product_ID|Gender|   Age|       Occupation|City_Category|Stay_In_Current_City_Years|     Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|         Purchase|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            550068|    550068|550068|550068|           550068|       550068|                    550068|             550068|            550068|            376430|            166821|           550068|\n",
      "|   mean|1003028.8424013031|      null|  null|  null|8.076706879876669|         null|         1.468494139793958|0.40965298835780306| 5.404270017525106| 9.842329251122386|12.668243206790512|9263.968712959126|\n",
      "| stddev|1727.5915855313278|      null|  null|  null|6.522660487341822|         null|        0.9890866807573164|0.49177012631733186| 3.936211369201384| 5.086589648693496|4.1253376315752845|5023.065393820554|\n",
      "|    min|           1000001| P00000142|     F|  0-17|                0|            A|                         0|                  0|                 1|                 2|                 3|               12|\n",
      "|    max|           1006040|  P0099942|     M|   55+|               20|            C|                        4+|                  1|                20|                18|                18|            23961|\n",
      "+-------+------------------+----------+------+------+-----------------+-------------+--------------------------+-------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#take a peak at the data\n",
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "test=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\projects\\test_HujdGe7\\test.csv')\n",
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|Purchase|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0|            173638|            383247|       0|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking for missing data\n",
    "from pyspark.sql.functions import col,sum\n",
    "train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Product_Category_2|Product_Category_3|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0|             72344|            162562|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "test.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in test.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product ID by itself is of not much advantage so we find the avg price of it.\n",
    "mean= train.groupBy(train.Product_ID).mean('Purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join will add the 2 column so cleaning up the names\n",
    "mean=mean.withColumnRenamed('avg(Purchase)', 'Avg')\n",
    "mean=mean.withColumnRenamed('Product_ID', 'Dup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New feature for avg spend of the user\n",
    "purchasehistory= train.groupBy(train.User_ID).mean('Purchase')\n",
    "#purchasehistory.show()\n",
    "purchasehistory=purchasehistory.withColumnRenamed('avg(Purchase)', 'Avg_spend')\n",
    "purchasehistory=purchasehistory.withColumnRenamed('User_ID', 'User_Dup')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the test set has product ID which the train set doesnt have\n",
    "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID'))\n",
    "diff_cat_in_train_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+--------+------------------+-----------------+\n",
      "|User_ID|Product_ID|Gender| Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Purchase|               Avg|        Avg_spend|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+--------+------------------+-----------------+\n",
      "|1000001| P00069042|     F|0-17|        10|            A|                         2|             0|                 3|    8370|11870.863436123347|9545.514285714286|\n",
      "|1000001| P00248942|     F|0-17|        10|            A|                         2|             0|                 1|   15200|16304.030981067126|9545.514285714286|\n",
      "|1000001| P00087842|     F|0-17|        10|            A|                         2|             0|                12|    1422|1237.8921568627452|9545.514285714286|\n",
      "+-------+----------+------+----+----------+-------------+--------------------------+--------------+------------------+--------+------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New features of avg price of each product and avg spend of each user\n",
    "train=train.join(mean,train.Product_ID==mean.Dup).join(purchasehistory,train.User_ID==purchasehistory.User_Dup)\n",
    "\n",
    "train=train.drop('Product_Category_2','Product_Category_3','Dup','User_Dup')\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+-----------------+------------------+\n",
      "|User_ID|Product_ID|Gender|  Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|              Avg|         Avg_spend|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+-----------------+------------------+\n",
      "|1000004| P00128942|     M|46-50|         7|            B|                         2|             1|                 1|15781.11858974359|14747.714285714286|\n",
      "|1000009| P00113442|     M|26-35|        17|            C|                         0|             0|                 3|11746.66535433071|10243.086206896553|\n",
      "|1000010| P00288442|     F|36-45|         1|            B|                        4+|             1|                 5|5731.338028169014| 9728.744394618834|\n",
      "+-------+----------+------+-----+----------+-------------+--------------------------+--------------+------------------+-----------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#repeating it for the test dataset, notice that the join 'left' will add Null values if the product ID is not present\n",
    "test=test.join(mean,test.Product_ID==mean.Dup, how='left').join(purchasehistory,test.User_ID==purchasehistory.User_Dup,how='left')\n",
    "test=test.drop('Product_Category_2','Product_Category_3','Dup','User_Dup')\n",
    "test.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---+---------+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Avg|Avg_spend|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---+---------+\n",
      "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0| 61|        0|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#recheck for missing values , 61 avg values need to be updated\n",
    "from pyspark.sql.functions import col,sum\n",
    "test.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in test.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null values of the Avg price of the Product ID that are not present in the train set are imputed\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Avg\"], \n",
    "    outputCols=[\"Avg_imputed\"]\n",
    ")\n",
    "df_change=imputer.fit(test).transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop and rename columns\n",
    "tester=df_change.drop('Avg','Dup')\n",
    "tester=tester.withColumnRenamed('Avg_imputed','Avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---------+---+\n",
      "|User_ID|Product_ID|Gender|Age|Occupation|City_Category|Stay_In_Current_City_Years|Marital_Status|Product_Category_1|Avg_spend|Avg|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---------+---+\n",
      "|      0|         0|     0|  0|         0|            0|                         0|             0|                 0|        0|  0|\n",
      "+-------+----------+------+---+----------+-------------+--------------------------+--------------+------------------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#recheck for missing values \n",
    "from pyspark.sql.functions import col,sum\n",
    "tester.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in tester.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert categorical columns\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "categorical_columns = ['Age','Gender','Occupation','City_Category']\n",
    "\n",
    "## build stages for string indexer and onehot encoding\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) for c in categorical_columns]\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "## fit pipeline model for train\n",
    "df_coded = pipeline.fit(train).transform(train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected features\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['Avg','Marital_Status','Avg_spend']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to features vector\n",
    "vectorassembler = VectorAssembler(inputCols=selected_columns, outputCol='features')\n",
    "\n",
    "# transform data\n",
    "df_features = vectorassembler.transform(df_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_mode = pipeline.fit(tester)\n",
    "\n",
    "## transform data\n",
    "test_coded = pipeline_mode.transform(tester)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = vectorassembler.transform(test_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression \n",
    "import pyspark.ml.regression as rg\n",
    "linalg= rg.LinearRegression(\n",
    "    maxIter=10, \n",
    "    regParam=0.01, \n",
    "    labelCol='Purchase')\n",
    "\n",
    "\n",
    "\n",
    "lrmodel = linalg.fit(df_features)\n",
    "resulttrain= lrmodel.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "resulttest = lrmodel.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233599"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ensure all rows as per the original test dataset is present\n",
    "resulttest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+\n",
      "|User_ID|Product_ID|         Purchase|\n",
      "+-------+----------+-----------------+\n",
      "|1000004| P00128942|17939.08638407769|\n",
      "|1000009| P00113442|11951.05908583899|\n",
      "+-------+----------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "submission =resulttest.select(col(\"User_ID\"),col(\"Product_ID\"),col(\"prediction\"))\n",
    "final=submission.withColumnRenamed('prediction','Purchase')\n",
    "final.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluatee on train\n",
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.RegressionEvaluator(\n",
    "    predictionCol='prediction', \n",
    "    labelCol='Purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2554.8615568235855\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(resulttrain, \n",
    "    {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error is Rs 2554,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
