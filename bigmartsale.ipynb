{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\bigmart\\Train_UWu5bXk.csv')\n",
    "\n",
    "#train.count()\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\bigmart\\Test_u94Q5KV.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[Item_Identifier: string, Item_Weight: double, Item_Fat_Content: string, Item_Visibility: double, Item_Type: string, Item_MRP: double, Outlet_Identifier: string, Outlet_Establishment_Year: int, Outlet_Size: string, Outlet_Location_Type: string, Outlet_Type: string, Item_Outlet_Sales: double]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|  Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|      Outlet_Type|Item_Outlet_Sales|\n",
      "+---------------+-----------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "|          FDA15|        9.3|         Low Fat|    0.016047301|      Dairy|249.8092|           OUT049|                     1999|     Medium|              Tier 1|Supermarket Type1|         3735.138|\n",
      "|          DRC01|       5.92|         Regular|    0.019278216|Soft Drinks| 48.2692|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|         443.4228|\n",
      "|          FDN15|       17.5|         Low Fat|    0.016760075|       Meat| 141.618|           OUT049|                     1999|     Medium|              Tier 1|Supermarket Type1|          2097.27|\n",
      "+---------------+-----------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+-----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|Outlet_Type|Item_Outlet_Sales|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+-----------------+\n",
      "|              0|       1463|               0|              0|        0|       0|                0|                        0|       2410|                   0|          0|                0|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|Outlet_Type|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+\n",
      "|              0|        976|               0|              0|        0|       0|                0|                        0|       1606|                   0|          0|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "test.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in test.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fill in missing data with Imputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"Item_Weight\"], \n",
    "    outputCols=[\"Weight_imputed\"]\n",
    ")\n",
    "df_change=imputer.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=imputer.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------+-----------------+-----------------+-----------------+\n",
      "|Outlet_Size_Outlet_Type|Grocery Store|Supermarket Type1|Supermarket Type2|Supermarket Type3|\n",
      "+-----------------------+-------------+-----------------+-----------------+-----------------+\n",
      "|                 Medium|            0|              620|              618|              624|\n",
      "|                   High|            0|              621|                0|                0|\n",
      "|                  Small|          352|             1240|                0|                0|\n",
      "|                   null|          370|             1236|                0|                0|\n",
      "+-----------------------+-------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.crosstab('Outlet_Size','Outlet_Type').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------+------+------+\n",
      "|Outlet_Size_Outlet_Location_Type|Tier 1|Tier 2|Tier 3|\n",
      "+--------------------------------+------+------+------+\n",
      "|                          Medium|   620|     0|  1242|\n",
      "|                            High|     0|     0|   621|\n",
      "|                           Small|   972|   620|     0|\n",
      "|                            null|     0|  1236|   370|\n",
      "+--------------------------------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tester.crosstab('Outlet_Size','Outlet_Location_Type').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----+------+-----+----+\n",
      "|Outlet_Identifier_Outlet_Size|High|Medium|Small|null|\n",
      "+-----------------------------+----+------+-----+----+\n",
      "|                       OUT045|   0|     0|    0| 929|\n",
      "|                       OUT013| 932|     0|    0|   0|\n",
      "|                       OUT049|   0|   930|    0|   0|\n",
      "|                       OUT046|   0|     0|  930|   0|\n",
      "|                       OUT027|   0|   935|    0|   0|\n",
      "|                       OUT035|   0|     0|  930|   0|\n",
      "|                       OUT017|   0|     0|    0| 926|\n",
      "|                       OUT010|   0|     0|    0| 555|\n",
      "|                       OUT019|   0|     0|  528|   0|\n",
      "|                       OUT018|   0|   928|    0|   0|\n",
      "+-----------------------------+----+------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_change.crosstab('Outlet_Identifier','Outlet_Size').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change=df_change.fillna('Small','Outlet_Size')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=tester.fillna('Small','Outlet_Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+--------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|Item_Visibility|Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|Outlet_Type|Weight_imputed|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+--------------+\n",
      "|              0|        976|               0|              0|        0|       0|                0|                        0|          0|                   0|          0|             0|\n",
      "+---------------+-----------+----------------+---------------+---------+--------+-----------------+-------------------------+-----------+--------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "tester.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in tester.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=tester.drop('Item_Weight')\n",
    "tester=tester.withColumnRenamed('Weight_imputed','Item_Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change=df_change.drop('Item_Weight')\n",
    "df_change=df_change.withColumnRenamed('Weight_imputed','Item_Weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "menaMRP=df_change.groupBy('Item_Identifier').mean('Item_MRP')\n",
    "menaMRP=menaMRP.withColumnRenamed('Item_Identifier','Dup')\n",
    "#menaMRP.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+-----------+-----+-------------+\n",
      "|Item_Identifier|Item_Fat_Content|Item_Visibility|  Item_Type|Item_MRP|Outlet_Identifier|Outlet_Establishment_Year|Outlet_Size|Outlet_Location_Type|      Outlet_Type|Item_Outlet_Sales|Item_Weight|  Dup|avg(Item_MRP)|\n",
      "+---------------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+-----------+-----+-------------+\n",
      "|          FDA15|         Low Fat|    0.016047301|      Dairy|249.8092|           OUT049|                     1999|     Medium|              Tier 1|Supermarket Type1|         3735.138|        9.3|FDA15|     249.4967|\n",
      "|          DRC01|         Regular|    0.019278216|Soft Drinks| 48.2692|           OUT018|                     2009|     Medium|              Tier 3|Supermarket Type2|         443.4228|       5.92|DRC01|      48.9692|\n",
      "+---------------+----------------+---------------+-----------+--------+-----------------+-------------------------+-----------+--------------------+-----------------+-----------------+-----------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "trainer=df_change.join(menaMRP, (df_change.Item_Identifier==menaMRP.Dup))\n",
    "\n",
    "trainer.show(2)\n",
    "#test=test.drop('Product_Category_2','Product_Category_3','Dup','User_Dup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=trainer.withColumnRenamed('avg(Item_MRP)','meanMRP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=trainer.withColumn('logmeanMRP',log(trainer.meanMRP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=trainer.withColumn('smallgrocery',(trainer.Outlet_Size=='Small')&(trainer.Outlet_Type=='Grocery store'))\n",
    "trainer=trainer.withColumn('smallsm1',(trainer.Outlet_Size=='Small')&(trainer.Outlet_Type=='Supermarket Type1'))\n",
    "trainer=trainer.withColumn('medsm1',(trainer.Outlet_Size=='Medium')&(trainer.Outlet_Type=='Supermarket Type1'))\n",
    "trainer=trainer.withColumn('highsm1',(trainer.Outlet_Size=='High')&(trainer.Outlet_Type=='Supermarket Type1'))\n",
    "trainer=trainer.withColumn('medsm2',(trainer.Outlet_Size=='Medium')&((trainer.Outlet_Type=='Supermarket Type2')|(trainer.Outlet_Type=='Supermarket Type3')))\n",
    "trainer=trainer.withColumn('storeage',(2013-trainer.Outlet_Establishment_Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=trainer.withColumn('Nosold',(trainer.Outlet_Size=='Medium')&(trainer.Outlet_Type=='Supermarket Type3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=tester.join(menaMRP,tester.Item_Identifier==menaMRP.Dup)\n",
    "tester=tester.withColumnRenamed('avg(Item_MRP)','meanMRP')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=tester.withColumn('logmeanMRP',log(tester.meanMRP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=tester.withColumn('smallgrocery',(tester.Outlet_Size=='Small')&(tester.Outlet_Type=='Grocery store'))\n",
    "tester=tester.withColumn('smallsm1',(tester.Outlet_Size=='Small')&(tester.Outlet_Type=='Supermarket Type1'))\n",
    "tester=tester.withColumn('medsm1',(tester.Outlet_Size=='Medium')&(tester.Outlet_Type=='Supermarket Type1'))\n",
    "tester=tester.withColumn('highsm1',(tester.Outlet_Size=='High')&(tester.Outlet_Type=='Supermarket Type1'))\n",
    "tester=tester.withColumn('medsm2',(tester.Outlet_Size=='Medium')&((tester.Outlet_Type=='Supermarket Type2')|(tester.Outlet_Type=='Supermarket Type3')))\n",
    "tester=tester.withColumn('storeage',(2013-tester.Outlet_Establishment_Year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_change.select('Outlet_Identifier').distinct().count())\n",
    "print(df_change.select('Item_Identifier').distinct().count())\n",
    "print(df_change.select('Item_MRP').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.select('Outlet_Identifier').distinct().count())\n",
    "print(test.select('Item_Identifier').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cat_in_train_test=test.select('Item_Identifier').subtract(df_change.select('Item_Identifier'))\n",
    "diff_cat_in_train_test.distinct().count()# For distict count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_change.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "categorical_columns = ['Item_Type','Outlet_Identifier']\n",
    "#categorical_columns = ['Outlet_Identifier']\n",
    "\n",
    "##=== build stages ======\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]\n",
    "\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) for c in categorical_columns]\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['Item_MRP','smallgrocery','smallsm1','medsm1','highsm1','medsm2','storeage']\n",
    "#selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['meanMRP']\n",
    "vectorassembler_stages = [VectorAssembler(inputCols=selected_columns, outputCol='features')]\n",
    "all_stages = stringindexer_stages + onehotencoder_stages+vectorassembler_stages\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "## fit pipeline model\n",
    "df_features = pipeline.fit(trainer).transform(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "testerfinal = pipeline.fit(tester).transform(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.RegressionEvaluator(\n",
    "    predictionCol='prediction', \n",
    "    labelCol='Item_Outlet_Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pyspark.ml.regression as rg\n",
    "linalg= rg.LinearRegression(labelCol='Item_Outlet_Sales')\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(linalg.maxIter,[10,100])\\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=linalg, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "trainresult=crossval.fit(df_features)\n",
    "# lrmodel = linalg.fit(df_features)\n",
    "# lrtmodel = lrmodel.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1132.1801123443342, 1132.6929980572781]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainresult.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lrmodel.transform(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "submission =testresult.select(col(\"Item_Identifier\"),col(\"Outlet_Identifier\"),col(\"prediction\"))\n",
    "final=submission.withColumnRenamed('prediction','Item_Outlet_Sales')\n",
    "final.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.write.csv('Bigmartsale9.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "submission=result.select(col(\"Product_ID\"),col(\"prediction\").cast('int'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"Item_Outlet_Sales\")\n",
    " \n",
    "model = rf.fit(df_features)\n",
    "rfpredictions = model.transform(df_features)\n",
    " \n",
    "#tpredictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159.5685004918219\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(rfpredictions, \n",
    "    {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(testerfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(labelCol=\"Item_Outlet_Sales\")\n",
    " \n",
    "gbtmodel = gbt.fit(df_features)\n",
    "gbtpredictions = gbtmodel.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=gbtmodel.transform(testerfinal)\n",
    "#result.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1039.8295419144301\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(gbtpredictions, \n",
    "    {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.evaluate(lrtmodel, \n",
    "    {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission =lrtmodel.select(col('User_ID'),col(\"Purchase\"),col(\"prediction\").cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"Item_Outlet_Sales\",featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxDepth,[2,5])\\\n",
    "    .addGrid(gbt.maxIter,[10,100])\\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "result=crossval.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1184.2675891963736,\n",
       " 1085.5291745345496,\n",
       " 1091.0529378820088,\n",
       " 1145.9881312091436]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainresult=result.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.evaluate(trainresult, \n",
    "    {evaluator.metricName: 'rmse'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testresult=result.transform(testerfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(labelCol=\"Item_Outlet_Sales\")\n",
    " \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth,[2,5])\\\n",
    "    .addGrid(rf.numTrees,[10,100])\\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "\n",
    "rfmodel = crossval.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
