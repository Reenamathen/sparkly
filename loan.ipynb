{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\loan\\train_u6lujuX_CVtuZ9i.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=spark.read.option('header','True').option('inferSchema','True').csv(r'C:\\spark_projects\\loan\\test_Y3wMUE5_7gLdaTN.csv')\n",
    "#test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "test.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in test.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.fillna({'Gender':'Male','Married':'Yes','Self_Employed':'No','Dependents':'0'})\n",
    "train=train.fillna({'Gender':'Male','Married':'Yes','Self_Employed':'No','Dependents':'0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "test=test.withColumn('LoanD', test.LoanAmount.cast('float'))\n",
    "test=test.withColumn('Loan_termD', test.Loan_Amount_Term.cast('float'))\n",
    "test=test.withColumn('Credit_D', test.Credit_History.cast('float'))\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"LoanD\",'Loan_termD','Credit_D'], \n",
    "    outputCols=[\"Avg_imputed\",'Amount_imp','Credit_History_imp']\n",
    ")\n",
    "\n",
    "test=imputer.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=train.dropna()\n",
    "train=train.withColumn('LoanD', train.LoanAmount.cast('float'))\n",
    "train=train.withColumn('Loan_termD', train.Loan_Amount_Term.cast('float'))\n",
    "train=train.withColumn('Credit_D', train.Credit_History.cast('float'))\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"LoanD\",'Loan_termD','Credit_D'], \n",
    "    outputCols=[\"Avg_imputed\",'Amount_imp','Credit_History_imp']\n",
    ")\n",
    "\n",
    "train=imputer.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "train=train.withColumn('MM',(train.Gender=='M') & ( train.Married=='Yes'))\n",
    "train=train.withColumn('MUM',(train.Gender=='M') & ( train.Married=='No'))\n",
    "train=train.withColumn('FM',(train.Gender=='F') & ( train.Married=='Yes'))\n",
    "train=train.withColumn('FUM',(train.Gender=='F') & ( train.Married=='No'))\n",
    "train=train.withColumn('Int',((train.LoanAmount) / ( train.Loan_Amount_Term))*1000)\n",
    "#train=train.withColumn('Ratio',((train.ApplicantIncome) / ( train.Int)))\n",
    "#train.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.withColumn('20k',(train.LoanAmount<=20000))\n",
    "train=train.withColumn('50k',(train.LoanAmount>=20000)&(train.LoanAmount<=50000))\n",
    "train=train.withColumn('>50k',(train.LoanAmount>=50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.withColumn('MM',(test.Gender=='M') & ( test.Married=='Yes'))\n",
    "test=test.withColumn('MUM',(test.Gender=='M') & ( test.Married=='No'))\n",
    "test=test.withColumn('FM',(test.Gender=='F') & ( test.Married=='Yes'))\n",
    "test=test.withColumn('FUM',(test.Gender=='F') & ( test.Married=='No'))\n",
    "test=test.withColumn('Int',((test.Avg_imputed) / ( test.Amount_imp))*1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.drop('LoanAmount')\n",
    "\n",
    "test=test.drop('Loan_Amount_Term')\n",
    "\n",
    "test=test.drop('Credit_History')\n",
    "train=train.drop('LoanAmount')\n",
    "\n",
    "train=train.drop('Loan_Amount_Term')\n",
    "\n",
    "train=train.drop('Credit_History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.withColumnRenamed('Avg_imputed','LoanAmount')\n",
    "test=test.withColumnRenamed('Amount_imp','Loan_Amount_Term')\n",
    "test=test.withColumnRenamed('Credit_History_imp','Credit_History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.withColumnRenamed('Avg_imputed','LoanAmount')\n",
    "train=train.withColumnRenamed('Amount_imp','Loan_Amount_Term')\n",
    "train=train.withColumnRenamed('Credit_History_imp','Credit_History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "test=test.withColumn('TotalIncome',(test.ApplicantIncome) + ( test.CoapplicantIncome))\n",
    "test=test.withColumn('LIncome',log(10.0,test.TotalIncome))\n",
    "test=test.withColumn('LogofLAmount',log(10.0,test.LoanAmount))\n",
    "train=train.withColumn('TotalIncome',(train.ApplicantIncome) + ( train.CoapplicantIncome))\n",
    "train=train.withColumn('LIncome',log(10.0,train.TotalIncome))\n",
    "train=train.withColumn('LogofLAmount',log(10.0,train.LoanAmount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+-------------+-----------+-----+----------+--------+----------+----------------+--------------+---+---+---+---+---+-----------+-------+------------+---+---+----+\n",
      "|Loan_ID|Gender|Married|Dependents|Education|Self_Employed|ApplicantIncome|CoapplicantIncome|Property_Area|Loan_Status|LoanD|Loan_termD|Credit_D|LoanAmount|Loan_Amount_Term|Credit_History| MM|MUM| FM|FUM|Int|TotalIncome|LIncome|LogofLAmount|20k|50k|>50k|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+-------------+-----------+-----+----------+--------+----------+----------------+--------------+---+---+---+---+---+-----------+-------+------------+---+---+----+\n",
      "|      0|     0|      0|         0|        0|            0|              0|                0|            0|          0|   22|        14|      50|         0|               0|             0|  0|  0|  0|  0| 36|          0|      0|           0| 22| 22|  22|\n",
      "+-------+------+-------+----------+---------+-------------+---------------+-----------------+-------------+-----------+-----+----------+--------+----------+----------------+--------------+---+---+---+---+---+-----------+-------+------------+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "train.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in train.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from pyspark.sql import *\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "categorical_columns = ['Dependents','Education','Self_Employed','Property_Area']\n",
    "#categorical_columns = ['Dependents','Property_Area']\n",
    "\n",
    "##=== build stages ======\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) for c in categorical_columns]\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['MM','FM','FUM','MUM','TotalIncome','LoanAmount','Credit_History','Loan_Amount_Term']\n",
    "\n",
    "vectorAssembler_stages = [VectorAssembler(inputCols=selected_columns, outputCol='features')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stages = stringindexer_stages + onehotencoder_stages +vectorAssembler_stages\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "## fit pipeline model\n",
    "final= pipeline.fit(train).transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfinal= pipeline.fit(test).transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoanGrant=StringIndexer(inputCol='Loan_Status', outputCol='Indexed')\n",
    "finaltrain= LoanGrant.fit(final).transform(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LogisticRegression(labelCol='Indexed',featuresCol='features')\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(lr.maxIter, [10, 100])\\\n",
    "  .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "\n",
    "lrmodel = crossval.fit(finaltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7707801781787584, 0.7580591644124155]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.780855055292259"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(lrmodel.transform(finaltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=lrmodel.transform(testfinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=results.withColumn('outputint',results.prediction.cast('int'))\n",
    "results=results.withColumn('output',results.outputint.cast('string'))\n",
    "\n",
    "converter = IndexToString(inputCol=\"outputint\", outputCol=\"Loan_status\",labels=['Y','N'])\n",
    "converted = converter.transform(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "| Loan_ID|Loan_Status|\n",
      "+--------+-----------+\n",
      "|LP001015|          Y|\n",
      "|LP001022|          Y|\n",
      "+--------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "submission =converted.select(col(\"Loan_ID\"),col(\"Loan_Status\"))\n",
    "submission.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.write.csv('LoanPredictiontrial2.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestPipeline = cvModel.bestModel\n",
    "bestLRModel = lrmodel.bestModel\n",
    "bestParams = bestLRModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='labelCol', doc='label column name'): 'Indexed', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='maxIter', doc='maximum number of iterations (>= 0)'): 10, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='regParam', doc='regularization parameter (>= 0)'): 0.0, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5, Param(parent='LogisticRegression_47c2b6ccd893a46a6f37', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "print(bestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Indexed\")\n",
    " \n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth,[2,5])\\\n",
    "    .addGrid(rf.numTrees,[10,100])\\\n",
    "    .addGrid(rf.featureSubsetStrategy,['all','sqrt'])\\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "\n",
    "rfmodel = crossval.fit(finaltrain)\n",
    " #.addGrid(rf.featureSubsetStrategy,['sqrt')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7448989033695762,\n",
       " 0.7458992722675601,\n",
       " 0.7586505532791029,\n",
       " 0.7692338452611442,\n",
       " 0.7607810695367393,\n",
       " 0.7468852537011978,\n",
       " 0.7687772208469259,\n",
       " 0.7682786790003096]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8288593503159565"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(rfmodel.transform(finaltrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"Indexed\",featuresCol=\"features\")\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(gbt.maxDepth,[2,5])\\\n",
    "    .addGrid(gbt.maxIter,[10,100])\\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(\n",
    "    estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=6)\n",
    "\n",
    "\n",
    "bgtmodel = crossval.fit(finaltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7545799853927881,\n",
       " 0.7557201477043854,\n",
       " 0.7344869975918898,\n",
       " 0.7482788278782198]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgtmodel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "evaluator = ev.BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='probability', \n",
    "    labelCol='Indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.evaluate(rfpredictions, \n",
    "    {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(rfpredictions, \n",
    "   {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rftestpredictions=rftestpredictions.withColumn('outputint',rftestpredictions.prediction.cast('int'))\n",
    "rftestpredictions=rftestpredictions.withColumn('output',rftestpredictions.outputint.cast('string'))\n",
    "rftestpredictions.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=result.withColumn('outputint',result.prediction.cast('int'))\n",
    "result=result.withColumn('output',result.outputint.cast('string'))\n",
    "\n",
    "\n",
    "converter = IndexToString(inputCol=\"outputint\", outputCol=\"Loan_status\",labels=['Y','N'])\n",
    "converted = converter.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "submission =converted.select(col(\"Loan_ID\"),col(\"Loan_Status\"))\n",
    "submission.show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.write.csv('LoanPrediction.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display CV score\n",
    "auc_roc = model.avgMetrics[0]\n",
    "print(\"AUC ROC = %g\" % auc_roc)\n",
    "gini = (2 * auc_roc - 1)\n",
    "print(\"GINI ~=%g\" % gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
